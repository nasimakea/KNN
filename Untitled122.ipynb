{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55AlJQvhOqkZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is a projection and how is it used in PCA?"
      ],
      "metadata": {
        "id": "6mYvOWB8Pceg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AacZ-MaRPeBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "- In the context of Principal Component Analysis (PCA), a projection refers to the process of transforming data points from a higher-dimensional space to a lower-dimensional subspace defined by the principal components. This transformation involves projecting the original data points onto the principal axes, which are the eigenvectors of the covariance matrix of the data."
      ],
      "metadata": {
        "id": "92mo-LatPrVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "-VI6UiATPuwf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris=load_iris()\n",
        "x=iris.data\n"
      ],
      "metadata": {
        "id": "AN-a-1ZAP97x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()"
      ],
      "metadata": {
        "id": "WOzi8mUsQFXD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaled=scaler.fit_transform(x)"
      ],
      "metadata": {
        "id": "Gxe7voeaQIcN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "kJ401ODzQOdw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA(n_components=2)\n"
      ],
      "metadata": {
        "id": "fYy3ejFEQUwu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pca=pca.fit_transform(x_scaled)"
      ],
      "metadata": {
        "id": "wOEbiHjYQbHD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pca[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4e4BUQbQf9L",
        "outputId": "e6731628-f593-4066-c885-01ecf62ceabf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.26470281,  0.4800266 ])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XamKTEOFQg85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
      ],
      "metadata": {
        "id": "XSZAl04hQr7N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_BKJXjZQs0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "- The optimization problem in Principal Component Analysis (PCA) revolves around finding the directions (principal components) along which the data has maximum variance. The goal of PCA is to reduce the dimensionality of the data while preserving as much variance as possible. Mathematically, PCA aims to find the eigenvectors of the covariance matrix corresponding to the largest eigenvalues."
      ],
      "metadata": {
        "id": "e0ha2TXYRXCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "S69_nzUPRbxF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris=load_iris()"
      ],
      "metadata": {
        "id": "UTsWFQbNRn0j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=iris.data\n"
      ],
      "metadata": {
        "id": "DnphBycQRppL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()"
      ],
      "metadata": {
        "id": "Lusex4YHRrjP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaled=scaler.fit_transform(x)"
      ],
      "metadata": {
        "id": "jBJIv-b_RvZ9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YiQo3VLfR3Yf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov_matrix=np.cov(x_scaled.T)"
      ],
      "metadata": {
        "id": "jp1hVUpGR6hL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)"
      ],
      "metadata": {
        "id": "NPwkw03jSBte"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnKin8s6SIoy",
        "outputId": "3b01af06-c972-4e10-cf0c-b47a66014e1e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.93808505, 0.9201649 , 0.14774182, 0.02085386])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN6hE3rDSMSa",
        "outputId": "9fd7cd2d-873a-45eb-b897-0a61c49ce0ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.52106591, -0.37741762, -0.71956635,  0.26128628],\n",
              "       [-0.26934744, -0.92329566,  0.24438178, -0.12350962],\n",
              "       [ 0.5804131 , -0.02449161,  0.14212637, -0.80144925],\n",
              "       [ 0.56485654, -0.06694199,  0.63427274,  0.52359713]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[idx]\n",
        "eigenvectors = eigenvectors[:, idx]"
      ],
      "metadata": {
        "id": "CZ4R6EtNSOD_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 2"
      ],
      "metadata": {
        "id": "NMZliiDNSUOZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principal_components = eigenvectors[:, :n_components]"
      ],
      "metadata": {
        "id": "Fc_AKOI2SXXK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pca = x_scaled.dot(principal_components)"
      ],
      "metadata": {
        "id": "rcYXAJV3SZXZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kEvJtn7HSbeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. What is the relationship between covariance matrices and PCA?"
      ],
      "metadata": {
        "id": "xQAL2CTGSiuA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_UsoiegSjxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:\n",
        "\n",
        "\n",
        "-  The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works and why it is effective in dimensionality reduction and feature extraction.\n",
        "\n",
        "  -Covariance Matrix:\n",
        "The covariance matrix is a square matrix that captures the relationships between pairs of variables in a dataset. Specifically, it quantifies how much two variables vary together. A positive covariance indicates that the variables tend to increase or decrease together, while a negative covariance indicates an inverse relationship.\n",
        "\n",
        "   - PCA and Covariance Matrix:\n",
        "In PCA, the covariance matrix plays a central role in determining the principal components. The principal components are the directions (eigenvectors) along which the data has maximum variance. The eigenvalues associated with these eigenvectors represent the amount of variance explained by each principal component.\n",
        "\n",
        "   - Variance and Covariance:\n",
        "The diagonal elements of the covariance matrix represent the variances of individual variables, while the off-diagonal elements represent the covariances between pairs of variables. Higher values on the diagonal and off-diagonal indicate stronger variability and correlation, respectively.\n",
        "\n",
        "  - PCA Optimization:\n",
        "PCA aims to find a set of orthogonal axes (principal components) such that when the data is projected onto these axes, the variance of the projected data is maximized. Mathematically, this is achieved by finding the eigenvectors of the covariance matrix that correspond to the largest eigenvalues. These eigenvectors represent the directions in which the data has the most spread or variability.\n",
        "\n",
        "   - Dimensionality Reduction:\n",
        "By selecting a subset of principal components (eigenvectors) based on their corresponding eigenvalues, PCA effectively reduces the dimensionality of the data while preserving the most important information. The selected principal components form a new feature space that captures the essential characteristics of the original data in a lower-dimensional representation.\n",
        "   - Interpretation:\n",
        "In practical terms, the covariance matrix helps PCA identify the dominant patterns or relationships among variables in the data. Variables that exhibit strong correlations or variability contribute more significantly to the principal components, while variables with weak correlations contribute less to the principal components and may be considered less important in the dimensionality reduction process.\n"
      ],
      "metadata": {
        "id": "wkp2dlQVa_YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YwCSmXtRbhaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. How does the choice of number of principal components impact the performance of PCA?"
      ],
      "metadata": {
        "id": "vJUU64lybm0b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__qY-av2bn24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "- The choice of the number of principal components in Principal Component Analysis (PCA) has a significant impact on the performance and outcomes of PCA. Here's how the number of principal components affects PCA:\n",
        "\n",
        "  -Dimensionality Reduction:\n",
        "\n",
        "PCA is primarily used for dimensionality reduction. The number of principal components chosen determines the dimensionality of the reduced space. For example, if you choose to retain 2 principal components out of a 10-dimensional dataset, the reduced dataset will be 2-dimensional.\n",
        "A higher number of principal components retains more information from the original data but may also introduce more noise and computational complexity.\n",
        "   - Variance Retention:\n",
        "\n",
        "The number of principal components chosen directly impacts the amount of variance retained from the original data. Each principal component captures a certain amount of variance in the data.\n",
        "Typically, a plot called the \"explained variance ratio\" plot is used to visualize how much variance each principal component explains. This plot helps in determining the optimal number of principal components to retain based on the desired level of variance retention.\n",
        "   - Information Loss:\n",
        "\n",
        "Choosing fewer principal components can lead to information loss, especially if important variance in the data is captured by components that are not retained.\n",
        "It's crucial to strike a balance between reducing dimensionality and retaining enough information to preserve the essential characteristics of the data.\n",
        "  - Computational Efficiency:\n",
        "\n",
        "Using fewer principal components can improve computational efficiency, as operations in the reduced-dimensional space are faster and require less memory.\n",
        "However, the computational cost of PCA itself is usually not significantly impacted by the number of principal components chosen, as the computation of eigenvectors/eigenvalues is done regardless of the chosen number of components.\n",
        "  - Overfitting and Generalization:\n",
        "\n",
        "Choosing too many principal components can lead to overfitting, where the model captures noise or irrelevant patterns from the data.\n",
        "On the other hand, choosing too few principal components may result in underfitting, where the model fails to capture important patterns or relationships in the data"
      ],
      "metadata": {
        "id": "XqcQwTnscPnp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpFc1dcwcbWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
      ],
      "metadata": {
        "id": "VPNUDKxJcr9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ndEgIEbcs7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "\n",
        "- PCA can be used in feature selection as a dimensionality reduction technique, which indirectly helps in selecting the most informative features. Here's how PCA can be applied for feature selection and the benefits it offers:\n",
        "\n",
        "  - Variance-Based Feature Selection:\n",
        "\n",
        "In PCA, the principal components are ordered based on the amount of variance they capture in the data. The first few principal components typically capture the most significant variance.\n",
        "By analyzing the explained variance ratio of each principal component, you can identify the components that explain the majority of variance in the data.\n",
        "Features that contribute most to these informative principal components are considered important for representing the data and can be selected for feature selection.\n",
        "  - Dimensionality Reduction:\n",
        "\n",
        "PCA reduces the dimensionality of the data by transforming it into a lower-dimensional space defined by the principal components.\n",
        "This reduction in dimensionality inherently performs feature selection by focusing on the features that contribute most to the variance in the data while discarding less informative features.\n",
        "Features that have little impact on the principal components (i.e., low variance or low covariance with other features) are effectively filtered out during the dimensionality reduction process.\n",
        "  - Benefits of PCA for Feature Selection:\n",
        "\n",
        "Noise Reduction: PCA can help in filtering out noisy features that add little value to the model's predictive power. By focusing on the principal components with high variance, PCA emphasizes the most informative aspects of the data.\n",
        "Collinearity Handling: PCA can handle collinear features by transforming them into orthogonal principal components. This reduces multicollinearity issues that can affect model performance.\n",
        "  -  Improved Model Performance: By selecting features based on their contribution to variance, PCA can lead to improved model performance by focusing on the most relevant and discriminative features.\n",
        "  -  Simplification of Models: Feature selection with PCA results in a simplified representation of the data, which can lead to simpler and more interpretable models.\n",
        "  -  Computational Efficiency: Reduced dimensionality after PCA leads to faster computations and lower memory requirements, which is beneficial for large datasets and complex models."
      ],
      "metadata": {
        "id": "R0QgJ0J3dRkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBwPiTF3dh7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. What are some common applications of PCA in data science and machine learning?"
      ],
      "metadata": {
        "id": "NwSIaYYbdmQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgXYwHHodnLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}